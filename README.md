# FlashAttention
This is an implementation of flash attention using [Triton](https://openai.com/index/triton/) based on [FlashAttention: Fast and Memory-Eï¬ƒcient Exact Attention
with IO-Awareness](https://arxiv.org/pdf/2205.14135) and [FlashAttention-2:
Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/pdf/2307.08691)
